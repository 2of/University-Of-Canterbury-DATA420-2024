{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def username():\n",
    "    \"\"\"Get username with any domain information removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'<li><a href=\"{sc.uiWebUrl}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username() + \" (jupyter)\"}</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    user = username()\n",
    "    \n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .master(\"spark://masternode2:7077\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{user}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.executor.memory\", f\"{worker_memory}g\")\n",
    "        .config(\"spark.driver.memory\", f\"{master_memory}g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.ui.port\", str(port))\n",
    "        .appName(user + \" (jupyter)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading https://files.pythonhosted.org/packages/a8/07/72953cf70e3bd3a24cbc3e743e6f8539abe6e3e6d83c3c0c83426eaffd39/plotly-5.18.0-py3-none-any.whl (15.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 15.6MB 84kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting packaging (from plotly)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/8e/8de486cbd03baba4deef4142bd643a3e7bbe954a784dc1bb17142572d127/packaging-21.3-py3-none-any.whl (40kB)\n",
      "\u001b[K    100% |████████████████████████████████| 40kB 826kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tenacity>=6.2.0 (from plotly)\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/b0/c23bd61e1b32c9b96fbca996c87784e196a812da8d621d8d04851f6c8181/tenacity-8.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3.6/site-packages (from packaging->plotly)\n",
      "Installing collected packages: packaging, tenacity, plotly\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/commands/install.py\", line 365, in run\n",
      "    strip_file_prefix=options.strip_file_prefix,\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/req/req_set.py\", line 789, in install\n",
      "    **kwargs\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/req/req_install.py\", line 854, in install\n",
      "    strip_file_prefix=strip_file_prefix\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/req/req_install.py\", line 1069, in move_wheel_files\n",
      "    strip_file_prefix=strip_file_prefix,\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/wheel.py\", line 345, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/wheel.py\", line 316, in clobber\n",
      "    ensure_dir(destdir)\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/utils/__init__.py\", line 83, in ensure_dir\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib64/python3.6/os.py\", line 220, in makedirs\n",
      "    mkdir(name, mode)\n",
      "PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.6/site-packages/packaging'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "!pip3 install plotly #no permission to install packages to env. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>nki38 (jupyter)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:4692\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>4692</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/users/home/nki38/spark-warehouse/</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>spark://masternode2:7077</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>mathmadslinux2p.canterbury.ac.nz</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>36141</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>app-20240428214606-0237</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>nki38 (jupyter)</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1714297565515</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Dderby.system.home=/tmp/nki38/spark/</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>nki38 (jupyter)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#You may increase your resources\n",
    "#up to 4 executors, 2 cores per executor, 4 GB of executor memory, and 4 GB of master memory.\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>nki38 (jupyter)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:4692\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>4692</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/users/home/nki38/spark-warehouse/</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>spark://masternode2:7077</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>mathmadslinux2p.canterbury.ac.nz</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>36141</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>app-20240428214606-0237</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>nki38 (jupyter)</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1714297565515</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Dderby.system.home=/tmp/nki38/spark/</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>nki38 (jupyter)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#You may increase your resources\n",
    "#up to 4 executors, 2 cores per executor, 4 GB of executor memory, and 4 GB of master memory.\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlimited\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "LIMITER = False\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", StringType(), False),\n",
    "    StructField(\"DATE\", IntegerType(), True),\n",
    "    StructField(\"ELEMENT\", StringType(), True),\n",
    "    StructField(\"VALUE\", DoubleType(), True),\n",
    "    StructField(\"MEASUREMENT FLAG\", StringType(), True),\n",
    "    StructField(\"QUALITY FLAG\", StringType(), True),\n",
    "    StructField(\"SOURCE FLAG\", StringType(), True),\n",
    "    StructField(\"OBSERVATION TIME\", StringType(), True) \n",
    "])\n",
    "\n",
    "if  LIMITER:\n",
    "    all_daily = spark.read.csv(\"hdfs:///data/ghcnd/daily/2023.csv.gz\", schema)\n",
    "    all_daily = all_daily.limit(1000)\n",
    "    print(\"limited\")\n",
    "else:\n",
    "    all_daily = spark.read.csv(\"hdfs:///data/ghcnd/daily\", schema)\n",
    "    \n",
    "    print (\"unlimited\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldwide_precip = all_daily.filter(F.col('ELEMENT') == 'PRCP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldwide_precip = worldwide_precip.withColumn(\"year\", F.substring(F.col(\"DATE\").cast(\"string\"), 1, 4))\n",
    "worldwide_precip = worldwide_precip.withColumn(\"country\", F.substring(F.col(\"ID\").cast(\"string\"), 1, 2))\n",
    "worldwide_precip.show(1)\n",
    "print(worldwide_precip.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"world_prcp.csv\"\n",
    "output_path = f\"hdfs:///user/nki38/outputs/ghcnd/{filename}\"\n",
    "avg_measurement_by_year_country = worldwide_precip.groupBy(\"year\", \"country\") \\\n",
    "    .agg(F.avg(\"VALUE\").alias(\"avg_measurement\"))\n",
    "avg_measurement_by_year_country.write.csv(output_path, header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------------------+\n",
      "|year|country|   avg_measurement|\n",
      "+----+-------+------------------+\n",
      "|2011|     CS|100.37719298245614|\n",
      "+----+-------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_measurement_by_year_country.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|max_measurement|\n",
      "+---------------+\n",
      "|         4361.0|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_rainfall = avg_measurement_by_year_country.agg(F.max(\"avg_measurement\").alias(\"max_measurement\"))\n",
    "most_rainfall.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------------+\n",
      "|year|country|avg_measurement|\n",
      "+----+-------+---------------+\n",
      "|2000|     EK|         4361.0|\n",
      "+----+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country = avg_measurement_by_year_country.filter(F.col('avg_measurement') ==  4361.0)\n",
    "country.show()\n",
    "\n",
    "#THE ONLY REASON FOR THIS IS THAT THE OTHER DF IS AVAILABLE AND I FORGOT TO ADD THE CCODE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = avg_measurement_by_year_country.limit(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyToLocal: `ghcnd/NZ_Stations_ANALYSIS.csv/_SUCCESS': File exists\n",
      "copyToLocal: `ghcnd/NZ_Stations_ANALYSIS.csv/part-00000-222e1ba6-cefe-42fa-94cd-819b864f1840-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/countries_with_counts.csv/_SUCCESS': File exists\n",
      "copyToLocal: `ghcnd/countries_with_counts.csv/part-00000-fef3636e-b1d0-49cd-84a5-84f09fa91cae-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/nz_stations_distance.csv/_SUCCESS': File exists\n",
      "copyToLocal: `ghcnd/nz_stations_distance.csv/part-00000-c1d4ab45-4643-490f-b8e8-16e0045bd781-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/rainfall_by_year_country.csv/_SUCCESS': File exists\n",
      "copyToLocal: `ghcnd/rainfall_by_year_country.csv/part-00000-19bfba1f-87ef-4adb-ba91-aa79469773d0-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/states_with_counts.csv/_SUCCESS': File exists\n",
      "copyToLocal: `ghcnd/states_with_counts.csv/part-00000-7a1392fc-c1e2-448b-aa73-2a25bfb76281-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/stations_augmented.csv/_SUCCESS': File exists\n",
      "copyToLocal: `ghcnd/stations_augmented.csv/part-00000-a07817c6-3a5d-4b71-aea4-b271de1cd29c-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/_SUCCESS': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00000-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00001-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00002-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00003-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00004-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00005-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00006-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00007-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00008-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00009-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00010-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00011-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00012-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00013-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00014-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00015-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00016-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00017-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00018-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00019-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00020-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00021-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00022-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00023-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00024-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00025-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00026-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00027-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00028-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00029-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00030-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n",
      "copyToLocal: `ghcnd/world_prcp.csv/part-00031-91fa581d-b9a4-4ad0-9b12-1941f5b8943e-c000.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -copyToLocal /user/nki38/outputs/ghcnd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 items\r\n",
      "drwxr-xr-x   - nki38 nki38          0 2024-04-29 09:20 /user/nki38/outputs/ghcnd/2023_rainfall\r\n",
      "drwxr-xr-x   - nki38 nki38          0 2024-04-28 16:08 /user/nki38/outputs/ghcnd/NZ_Stations_ANALYSIS.csv\r\n",
      "drwxr-xr-x   - nki38 nki38          0 2024-04-28 11:15 /user/nki38/outputs/ghcnd/countries_with_counts.csv\r\n",
      "drwxr-xr-x   - nki38 nki38          0 2024-04-28 11:15 /user/nki38/outputs/ghcnd/nz_stations_distance.csv\r\n",
      "drwxr-xr-x   - nki38 nki38          0 2024-04-27 20:20 /user/nki38/outputs/ghcnd/rainfall_by_year_country.csv\r\n",
      "drwxr-xr-x   - nki38 nki38          0 2024-04-28 21:04 /user/nki38/outputs/ghcnd/rainfall_by_year_country_new.csv\r\n",
      "drwxr-xr-x   - nki38 nki38          0 2024-04-28 11:15 /user/nki38/outputs/ghcnd/states_with_counts.csv\r\n",
      "drwxr-xr-x   - nki38 nki38          0 2024-04-26 10:54 /user/nki38/outputs/ghcnd/stations_augmented\r\n",
      "drwxr-xr-x   - nki38 nki38          0 2024-04-26 11:09 /user/nki38/outputs/ghcnd/stations_augmented.csv\r\n",
      "drwxr-xr-x   - nki38 nki38          0 2024-04-28 21:55 /user/nki38/outputs/ghcnd/world_prcp.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/nki38/outputs/ghcnd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n",
      "+----+-------+------------------+\n",
      "|year|country|   avg_measurement|\n",
      "+----+-------+------------------+\n",
      "|2023|     AR| 50.20529482551143|\n",
      "|2023|     CA| 23.39627178903699|\n",
      "|2023|     EZ| 22.33065326633166|\n",
      "|2023|     FM|115.08110944527736|\n",
      "|2023|     NS|  59.4951768488746|\n",
      "|2023|     CU| 79.10144927536231|\n",
      "|2023|     ET| 73.85454545454546|\n",
      "|2023|     GG|21.219600725952812|\n",
      "|2023|     GI| 8.418732782369146|\n",
      "|2023|     MQ|17.701408450704225|\n",
      "|2023|     KG| 53.66942148760331|\n",
      "|2023|     IV| 88.92255236239649|\n",
      "|2023|     CD| 75.08196721311475|\n",
      "|2023|     PU| 119.5677966101695|\n",
      "|2023|     ZI| 69.11235955056179|\n",
      "|2023|     AE|3.9089635854341735|\n",
      "|2023|     AG| 7.280195274831244|\n",
      "|2023|     FJ|63.269992663242846|\n",
      "|2023|     MV| 8.582978723404254|\n",
      "|2023|     PS|112.04419101924448|\n",
      "+----+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2023 = avg_measurement_by_year_country.filter(F.col(\"year\")  == \"2023\")\n",
    "print(df_2023.count())\n",
    "df_2023.cache()\n",
    "df_2023.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------------------+\n",
      "|year|country|   avg_measurement|\n",
      "+----+-------+------------------+\n",
      "|2023|     AR| 50.20529482551143|\n",
      "|2023|     CA| 23.39627178903699|\n",
      "|2023|     EZ| 22.33065326633166|\n",
      "|2023|     FM|115.08110944527736|\n",
      "|2023|     NS|  59.4951768488746|\n",
      "|2023|     CU| 79.10144927536231|\n",
      "|2023|     ET| 73.85454545454546|\n",
      "|2023|     GG|21.219600725952812|\n",
      "|2023|     GI| 8.418732782369146|\n",
      "|2023|     MQ|17.701408450704225|\n",
      "|2023|     KG| 53.66942148760331|\n",
      "|2023|     IV| 88.92255236239649|\n",
      "|2023|     CD| 75.08196721311475|\n",
      "|2023|     PU| 119.5677966101695|\n",
      "|2023|     ZI| 69.11235955056179|\n",
      "|2023|     AE|3.9089635854341735|\n",
      "|2023|     AG| 7.280195274831244|\n",
      "|2023|     FJ|63.269992663242846|\n",
      "|2023|     MV| 8.582978723404254|\n",
      "|2023|     PS|112.04419101924448|\n",
      "+----+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2023.show()\n",
    "df_2023.repartition(1).write.csv( \"/user/nki38/outputs/ghcnd/2023_rainfall\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_spark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
